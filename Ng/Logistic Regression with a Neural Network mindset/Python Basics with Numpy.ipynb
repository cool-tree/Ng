{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c79ae8",
   "metadata": {},
   "source": [
    "Exercise: Set test to \"Hello World\" in the cell below to print \"Hello World\" and run the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9df1911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: hello world\n"
     ]
    }
   ],
   "source": [
    "test = \"hello world\"\n",
    "print(\"test: \"+test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2e188",
   "metadata": {},
   "source": [
    "Numpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). \n",
    "To refer to a function belonging to a specific package you could call it using package_name.function()\n",
    "when need more information on numpy function, look at the official documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d727fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basic functions with numpy\n",
    "#sigmoid function, np.exp():why np.exp() is preferable to math.exp():Actually, we rarely use the \"math\" library in deep learning because the inputs of the functions are real numbers. \n",
    "#In deep learning we mostly use matrices and vectors. This is why numpy is more useful.\n",
    "import math\n",
    "def basic_sigmoid(x):\n",
    "    s = 1/(1+math.exp(-x))\n",
    "    \n",
    "    return s\n",
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67e12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x= [1,2,3]\n",
    "#basic_sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd399b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,2,3])\n",
    "print(np.exp(x))# result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf532389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "#if x is a vector, then a Python operation such as s=x+3 or s=1/x will output s as a vector of the same size as x\n",
    "x = np.array([1,2,3])\n",
    "print(x+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22530ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implement the sigmoid function using numpy\n",
    "import numpy as np# this means you can access numpy functions by writing np.function() instead of numpy.function()\n",
    "def sigmoid(x):\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "x = np.array([1,2,3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad51af4",
   "metadata": {},
   "source": [
    "$s=\\frac{1}{1+e^{-x}} $;\n",
    "\n",
    "$\\frac{1}{s}=1+e^{-x};$\n",
    "\n",
    "$\\frac{s'}{s^2}=-e^{-x}=1-\\frac{1}{s};$\n",
    "\n",
    "$s'=s(s-1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0b54eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x)=[0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "#sigmoid gradient\n",
    "def sigmoid_derivative(x):\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    ds = s*(1-s)\n",
    "    return ds\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "print(\"sigmoid_derivative(x)=\"+str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9a4ea",
   "metadata": {},
   "source": [
    "Reshaping arrays:\n",
    "- X.shape is used to get the shape (dimension) of a matrix/vector X.\n",
    "- X.reshape(...) is used to reshape X into some other dimension.\n",
    "For example, in computer science, an image is represented by a 3D array of shape $(length,height,depth=3)$ . However, when you read an image as the input of an algorithm you convert it to a vector of shape $(length*height*3,1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc5f5ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "def image2vector(image):\n",
    "    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2],1))\n",
    "    return v\n",
    "\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \"+str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f8251",
   "metadata": {},
   "source": [
    "Normalizing rows\n",
    "\n",
    "It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $\\frac{x}{\\|x\\|}$(dividing each row vector of x by its norm)\n",
    "For example, if\n",
    "$$\n",
    "x=\\left[\\begin{array}{lll}\n",
    "0 & 3 & 4 \\\\\n",
    "2 & 6 & 4\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\|x\\|=\\text { np.linalg. norm }(x, \\text { axis }=1, \\text { keepdims }=\\text { True })=\\left[\\begin{array}{c}\n",
    "5 \\\\\n",
    "\\sqrt{56}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "and\n",
    "$$\n",
    "x_{-} \\text {normalized }=\\frac{x}{\\|x\\|}=\\left[\\begin{array}{ccc}\n",
    "0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "\\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "you can divide matrices of different sizes and it works fine, this is called broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab006dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "def normalizeRows(x):\n",
    "    x_norm = np.linalg.norm(x,ord = 2, axis = 1,keepdims = True)\n",
    "    x=x/x_norm\n",
    "\n",
    "    return x\n",
    "\n",
    "x = np.array([\n",
    "    [0,3,4],\n",
    "    [1,6,4]\n",
    "])\n",
    "print(\"normalizeRows(x) = \"+str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d6305",
   "metadata": {},
   "source": [
    "#Broadcasting and the softmax function\n",
    "- for $x \\in \\mathbb{R}^{1 \\times n}, \\operatorname{softmax}(x)=\\operatorname{softmax}\\left(\\left[\\begin{array}{llll}x_{1} & x_{2} & \\ldots & x_{n}\\end{array}\\right]\\right)=\\left[\\begin{array}{llll}\\frac{e^{x_{1}}}{\\sum_{j} e^{x_{j}}} & \\frac{e^{x_{2}}}{\\sum_{j} e^{x_{j}}} & \\cdots & \\frac{e^{x_{n}}}{\\sum_{j} e^{x_{j}}}\\end{array}\\right]$\n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{, $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$ \n",
    "$$\n",
    "softmax(x) = softmax\\begin{bmatrix} \n",
    "    x_{11} & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots & x_{mn} \\end{bmatrix} \n",
    "           = \\begin{bmatrix} \n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\ \n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}} \\end{bmatrix} \n",
    "           = \\begin{pmatrix} \n",
    "           softmax\\text{(first row of x)} \\\\ \n",
    "           softmax\\text{(second row of x)} \\\\\n",
    "           ... \\\\ \n",
    "           softmax\\text{(last row of x)}  \\end{pmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b7e025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp,axis = 1, keepdims = True)\n",
    "    s = x_exp/x_sum #x_exp/x_sum works due to python broadcasting.\n",
    "    return s\n",
    "\n",
    "x = np.array([\n",
    "    [9,2,5,0,0],\n",
    "    [7,5,0,0,0]\n",
    "])\n",
    "print(\"softmax(x) = \"+str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb02937",
   "metadata": {},
   "source": [
    "In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe770372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot =278\n",
      " ----- Computation time = 0.25948100000006136ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print(\"dot =\"+str(dot)+\"\\n ----- Computation time = \"+str(1000*(toc-tic))+\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de16b9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " ----- Computation time = 0.5271730000000474ms\n"
     ]
    }
   ],
   "source": [
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2)))\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "\n",
    "toc = time.process_time()\n",
    "print(\"outer = \" + str(outer)+\"\\n ----- Computation time = \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcc4e5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
      " ----- Computation time = 0.1910570000001055ms\n"
     ]
    }
   ],
   "source": [
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print(\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e368a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdot = [16.5916615  26.4456432  12.60284021]\n",
      " ----- Computation time = 0.4963190000000228ms\n"
     ]
    }
   ],
   "source": [
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) #Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]#矩阵向量点积\n",
    "toc = time.process_time()\n",
    "print(\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f55876c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.15933599999984338ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[54, 12, 30,  0,  0, 42, 30,  0,  0,  0, 54, 12, 30,  0,  0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "#x1*x2#can't multiply sequence by non-int of type 'list'\n",
    "#6*x1#注意列表的乘法，并不是每个值乘6，而是重复六遍\n",
    "#np.dot(6,x1)#每个值乘6\n",
    "#np.outer(6,x1)#注意结果是外积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "711561c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.26773300000004774ms\n"
     ]
    }
   ],
   "source": [
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print(\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1651eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.10617200000018201ms\n"
     ]
    }
   ],
   "source": [
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print(\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feefb657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdot = [16.5916615  26.4456432  12.60284021]\n",
      " ----- Computation time = 1.543985000000081ms\n"
     ]
    }
   ],
   "source": [
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print(\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02aee8",
   "metadata": {},
   "source": [
    "As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger.\n",
    "\n",
    "Note that np.dot() performs a matrix-matrix or matrix-vector multiplication. This is different from np.multiply() and the * operator (which is equivalent to .* in Matlab/Octave), which performs an element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19111e8e",
   "metadata": {},
   "source": [
    "- Implement the L1 and L2 loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cda1d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "def L1(yhat, y):\n",
    "    loss = sum(abs(y-yhat))\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "527dc522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "def L2(yhat, y):\n",
    "    loss = sum((y-yhat)**2)\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7dbfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
